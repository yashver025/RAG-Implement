# ğŸ“„ ChatDoc â€” RAG-based PDF Question Answering App

ChatDoc is a **Retrieval-Augmented Generation (RAG)** application built with **Streamlit, LangChain, Hugging Face, and FAISS**.
It allows users to upload a PDF document and ask natural-language questions, with answers generated **only from the document content**.

---

## ğŸš€ Features

* ğŸ“‚ Upload and process PDF documents
* âœ‚ï¸ Chunk documents using Recursive Character Text Splitting
* ğŸ§  Generate embeddings with Sentence Transformers
* ğŸ“¦ Store and retrieve vectors using FAISS
* ğŸ¤– Answer questions using Hugging Face LLMs (chat models)
* âš¡ Efficient caching to avoid re-processing and re-embedding
* ğŸ§ª Streamlit-based interactive UI

---

## ğŸ—ï¸ Architecture (RAG Pipeline)

```
PDF Upload
   â†“
Document Loader (PyPDFLoader)
   â†“
Text Splitter (RecursiveCharacterTextSplitter)
   â†“
Embeddings (sentence-transformers/all-MiniLM-L6-v2)
   â†“
Vector Store (FAISS)
   â†“
Retriever
   â†“
LLM (meta-llama/Llama-3.1-8B-Instruct)
   â†“
Answer
```

---

## ğŸ§° Tech Stack

* **Frontend:** Streamlit
* **LLM Orchestration:** LangChain
* **Embeddings:** Sentence Transformers
* **Vector Database:** FAISS
* **LLM Provider:** Hugging Face Inference API
* **Language:** Python 3.9+

---

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ app.py              # Streamlit application
â”œâ”€â”€ rag.py              # RAG pipeline logic
â”œâ”€â”€ rag_demo.ipynb      # Notebook for experimentation
â”œâ”€â”€ .env                # Environment variables (not committed)
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/your-username/chatdoc-rag.git
cd chatdoc-rag
```

---

### 2ï¸âƒ£ Create and activate virtual environment

```bash
python -m venv myenv
source myenv/bin/activate   # macOS/Linux
myenv\Scripts\activate      # Windows
```

---

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Configure environment variables

Create a `.env` file in the project root:

```env
HUGGINGFACEHUB_API_TOKEN=your_huggingface_token_here
```



---

### 5ï¸âƒ£ Run the application

```bash
streamlit run app.py
```

Open your browser at:

```
http://localhost:8501
```

---

## ğŸ§ª How It Works

1. Upload a PDF file
2. The document is split into chunks and embedded **once**
3. Embeddings are cached to avoid reloading
4. FAISS retrieves relevant chunks for each query
5. The LLM answers using **only the retrieved context**

---

## ğŸ›‘ Limitations

* Answers are limited to the content of the uploaded PDF
* Requires internet access for Hugging Face Inference API
* Large PDFs may take longer to process

---

## ğŸ”® Future Improvements

* ğŸ”— Source citations with page numbers
* ğŸ’¬ Multi-turn conversational memory
* ğŸ’¾ Persistent FAISS storage
* ğŸ–¥ï¸ Local LLM support (Ollama)
* â˜ï¸ Cloud deployment

---

## ğŸ‘¤ Author

**Yash**
IT Engineering Student
AI / ML Enthusiast
